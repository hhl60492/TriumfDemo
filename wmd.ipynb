{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Word Mover's Distance and application**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part I: Load 20NG dataset and grab ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\root\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\root\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "download('punkt')\n",
    "download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    doc = word_tokenize(text)\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    doc = [word for word in doc if word.isalpha()]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 docs removed\n"
     ]
    }
   ],
   "source": [
    "def filter_docs(corpus, texts, labels, condition_on_doc):\n",
    "    \"\"\"\n",
    "    Filter corpus, texts and labels given the function condition_on_doc which takes\n",
    "    a doc.\n",
    "    The document doc is kept if condition_on_doc(doc) is true.\n",
    "    \"\"\"\n",
    "    number_of_docs = len(corpus)\n",
    "\n",
    "    if texts is not None:\n",
    "        texts = [text for (text, doc) in zip(texts, corpus)\n",
    "                 if condition_on_doc(doc)]\n",
    "\n",
    "    labels = [i for (i, doc) in zip(labels, corpus) if condition_on_doc(doc)]\n",
    "    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n",
    "\n",
    "    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n",
    "\n",
    "    return (corpus, texts, labels)\n",
    "\n",
    "snippets = []\n",
    "snippets_labels = []\n",
    "snippets_file = \"data-web-snippets\\\\train.txt\"\n",
    "with open(snippets_file, 'r', encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        # each line is a snippet: a bag of words separated by spaces and\n",
    "        # the category\n",
    "        line = line.split()\n",
    "        category = line[-1]\n",
    "        doc = line[:-1]\n",
    "        snippets.append(doc)\n",
    "        snippets_labels.append(category)\n",
    "\n",
    "snippets, _, snippets_labels = filter_docs(snippets, None, snippets_labels,\n",
    "                                           lambda doc: (len(doc) != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part II: Compute the various similiarties between the snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we use Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\root\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import LsiModel\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "\n",
    "sims = {'snippets': {}}\n",
    "dictionary_snippets = corpora.Dictionary(snippets)\n",
    "corpus_gensim_snippets = [dictionary_snippets.doc2bow(doc) for doc in snippets]\n",
    "tfidf_snippets = TfidfModel(corpus_gensim_snippets)\n",
    "corpus_tfidf_snippets = tfidf_snippets[corpus_gensim_snippets]\n",
    "lsi_snippets = LsiModel(corpus_tfidf_snippets,\n",
    "                        id2word=dictionary_snippets, num_topics=200)\n",
    "lsi_index_snippets = MatrixSimilarity(lsi_snippets[corpus_tfidf_snippets])\n",
    "sims['snippets']['LSI'] = np.array([lsi_index_snippets[lsi_snippets[corpus_tfidf_snippets[i]]]\n",
    "                                    for i in range(len(snippets))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use Word2vec centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 docs removed\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "filename = 'C:\\\\GoogleNews-vectors-negative300.bin'\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "word2vec_model.init_sims(replace=True)\n",
    "def document_vector(word2vec_model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.vocab]\n",
    "    return np.mean(word2vec_model[doc], axis=0)\n",
    "\n",
    "def has_vector_representation(word2vec_model, doc):\n",
    "    \"\"\"check if at least one word of the document is in the\n",
    "    word2vec dictionary\"\"\"\n",
    "    return not all(word not in word2vec_model.vocab for word in doc)\n",
    "\n",
    "snippets, _, snippets_labels = filter_docs(snippets, None, snippets_labels,\n",
    "                                           lambda doc: has_vector_representation(word2vec_model, doc))\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sims['snippets']['centroid'] = cosine_similarity(np.array([document_vector(word2vec_model, doc)\n",
    "                                                           for doc in snippets]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use pairwise WMD distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "A = np.array([[i] for i in range(len(snippets))])\n",
    "\n",
    "def f(x, y):\n",
    "    return word2vec_model.wmdistance(snippets[int(x)], snippets[int(y)])\n",
    "\n",
    "X_wmd_distance_snippets = pairwise_distances(A, metric=f, n_jobs=-1)\n",
    "\n",
    "print(X_wmd_distance_snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSI distances\n[   0   13  973    2   14  901    5   15   17   16 6820 6827    7 7513    4\n 6831 6832   19  623   12]\ncentroid distances\n[   0   13   15  973  378   17    2 6658 6829 6833 6307   16  974    8 5535\n   19   14    5  965   12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WmdSimilarity<10056 docs, 300 features>\n[(0, 1.0), (13, 0.60329492929644768), (973, 0.5286272334018054), (2, 0.52601643645020979), (378, 0.52323760224834481), (16, 0.5177545982856413), (7509, 0.51254049902118537), (12, 0.51098133962722492), (6828, 0.50705399681485708), (19, 0.50653771271015224), (17, 0.50652784909122306), (974, 0.50597894422453937), (7, 0.50416865321814652), (15, 0.5041521835206122), (712, 0.50320841421252649), (6663, 0.50302378894798072), (56, 0.50239767268269631), (6829, 0.5021651306706193), (5, 0.501937836425843), (2169, 0.50181756077408601)]\nOriginal Snippet: \n['manufacture', 'manufacturer', 'directory', 'directory', 'china', 'taiwan', 'products', 'manufacturers', 'directory-', 'taiwan', 'china', 'products', 'manufacturer', 'direcory', 'exporter', 'directory', 'supplier', 'directory', 'suppliers']\n"
     ]
    }
   ],
   "source": [
    "def most_similar(i, X_sims, topn=None):\n",
    "    \"\"\"return the indices of the topn most similar documents with document i\n",
    "    given the similarity matrix X_sims\"\"\"\n",
    "\n",
    "    r = np.argsort(X_sims[i])[::-1]\n",
    "    if r is None:\n",
    "        return r\n",
    "    else:\n",
    "        return r[:topn]\n",
    "\n",
    "#LSI\n",
    "print(\"LSI distances\")\n",
    "print(most_similar(0, sims['snippets']['LSI'], 20))\n",
    "\n",
    "#Centroid\n",
    "print(\"centroid distances\")\n",
    "print(most_similar(0, sims['snippets']['centroid'], 20))\n",
    "\n",
    "from gensim.similarities import WmdSimilarity\n",
    "\n",
    "wmd_similarity_snippets = WmdSimilarity(snippets, word2vec_model, num_best=20)\n",
    "most_similars_snippets = wmd_similarity_snippets[snippets[0]]\n",
    "\n",
    "print(wmd_similarity_snippets)\n",
    "print(most_similars_snippets)\n",
    "print(\"Original Snippet: \")\n",
    "print(snippets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snippet match, WMD 1.0\n['manufacture', 'manufacturer', 'directory', 'directory', 'china', 'taiwan', 'products', 'manufacturers', 'directory-', 'taiwan', 'china', 'products', 'manufacturer', 'direcory', 'exporter', 'directory', 'supplier', 'directory', 'suppliers']\nSnippet match, WMD 0.603294929296\n['allproducts', 'allproducts', 'com', 'manufacturers', 'directory', 'products', 'database', 'global', 'marketplace', 'manufacture', 'directory', 'exporters', 'importers', 'wholesalers', 'volume', 'buyers', 'suppliers', 'product', 'directory']\nSnippet match, WMD 0.528627233402\n['wand', 'wand', 'com', 'directory', 'suppliers', 'traders', 'products', 'wand', 'com', 'directory', 'suppliers', 'traders', 'providers', 'products', 'services', 'buyers', 'wand', 'com', 'source', 'commodities']\nSnippet match, WMD 0.52601643645\n['dfma', 'truecost', 'paper', 'true', 'cost', 'overseas', 'manufacture', 'product', 'design', 'costs', 'manufacturing', 'products', 'china', 'manufacturing', 'redesigned', 'product', 'china', 'save']\nSnippet match, WMD 0.523237602248\n['erealdeal', 'erealdeal', 'marketplace-', 'trade', 'export', 'import', 'directory', 'auction', 'international', 'business', 'trade', 'import', 'export', 'portal', 'consumers', 'companies', 'manufacturers', 'suppliers', 'products', 'export', 'import']\nSnippet match, WMD 0.517754598286\n['inventionhome', 'inventor', 'services', 'manufacturing', 'overview', 'invention', 'manufacturing', 'inventors', 'manufacture', 'building', 'developing', 'invention', 'product', 'manufacturing', 'partnerships', 'china', 'taiwan']\nSnippet match, WMD 0.512540499021\n['onlinedentists', 'dental', 'products', 'business', 'directory', 'dental', 'resources', 'dental', 'products', 'business', 'directory', 'classifieds', 'resources', 'dentists', 'dental', 'professionals']\nSnippet match, WMD 0.510981339627\n['hyperlinktech', 'web', 'oem', 'products', 'hyperlink', 'technologies', 'equipment', 'manufacturer', 'oem', 'products', 'hyperlink', 'equipment', 'manufacturers', 'oem', 'solutions', 'design', 'product', 'adaptive', 'intuitive', 'innovative', 'creative']\nSnippet match, WMD 0.507053996815\n['tnb', 'betts', 'betts', 'manufacturer', 'electrical', 'hvac', 'lighting', 'utility', 'products']\nSnippet match, WMD 0.50653771271\n['firstresearch', 'industry', 'research', 'paper', 'products', 'manufacture', 'paper', 'products', 'manufacture', 'industry', 'paper', 'products', 'manufacture', 'industry', 'research', 'specializing', 'paper', 'products', 'manufacture', 'industry', 'research', 'paper', 'products', 'manufacture', 'market', 'research', 'paper', 'products']\nSnippet match, WMD 0.506527869758\n['marketresearch', 'product', 'display', 'productid', 'paper', 'products', 'manufacture', 'paper', 'products', 'manufacture', 'companies', 'manufacture', 'paper', 'products', 'annual', 'product', 'revenue', 'billion']\nSnippet match, WMD 0.505978944225\n['global', 'trade', 'global', 'trade', 'lead', 'global', 'directory', 'machine', 'manufacturer', 'giant', 'information', 'ltd', 'leading', 'manufacturer', 'specialized', 'machine', 'manufacturer', 'global', 'trade', 'lead', 'global', 'directory']\nSnippet match, WMD 0.504168653218\n['pentairpool', 'welcome', 'pentair', 'pool', 'products', 'inc', 'manufacturer', 'manufacturer', 'filters', 'sanitizers', 'cleaners', 'heaters', 'pumps', 'california']\nSnippet match, WMD 0.504152183521\n['partnerproducts', 'autodesk', 'catalog', 'industry', 'industry', 'mfg', 'autodesk', 'partner', 'products', 'services', 'manufacturing', 'outsourcing', 'mass', 'customization', 'global', 'competitiveness', 'reshaping', 'industry', 'design', 'manufacture', 'products']\nSnippet match, WMD 0.503208414213\n['dmoz', 'business', 'directory', 'business', 'directory', 'project', 'partnership', 'aol', 'search', 'dmoz', 'report', 'abuse', 'spam', 'help', 'directory', 'business']\nSnippet match, WMD 0.503023788948\n['made', 'china', 'machinery', 'catalog', 'motor', 'engine', 'china', 'motor', 'engine', 'motor', 'engine', 'catalog', 'china', 'motor', 'engine', 'china', 'motor', 'engine', 'catalog', 'motor', 'engine', 'manufacturer', 'directory', 'trade', 'platform', 'china', 'motor', 'engine', 'manufacturers', 'global', 'motor', 'engine']\nSnippet match, WMD 0.502397672683\n['com', 'directory', 'businesses', 'jobs', 'press', 'releases', 'products', 'comprehensive', 'directory', 'businesses', 'jobs', 'products', 'services', 'press', 'releases', 'news', 'articles', 'industries', 'promote', 'business']\nSnippet match, WMD 0.502165140475\n['hpm', 'electrical', 'products', 'electrical', 'accessories', 'home', 'commercial', 'hpm', 'industries', 'wholly', 'australian-owned', 'manufacturer', 'distributor', 'electrical', 'products', 'electronic', 'devices', 'products']\nSnippet match, WMD 0.501937836426\n['epa', 'gov', 'wastewise', 'wrr', 'buyman', 'buy', 'manufacture', 'recycled', 'wastewise', 'epa', 'organizations', 'manufactured', 'products', 'recycled', 'content', 'percentage', 'recycled', 'content', 'manufactured', 'products']\nSnippet match, WMD 0.501817560774\n['findanisp', 'isp', 'directory', 'helping', 'consumers', 'isps', 'isp', 'directory', 'dialup', 'access', 'providers', 'united']\n"
     ]
    }
   ],
   "source": [
    "for s in most_similars_snippets:\n",
    "    print(\"Snippet match, WMD \" + str(s[1]))\n",
    "    print(snippets[s[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}